{"title":"Measuring Ethical Risks in AI-Generated News Using NLP","markdown":{"yaml":{"title":"Measuring Ethical Risks in AI-Generated News Using NLP","index":5,"summary":"Built an NLP pipeline using DistilBERT, Transformers, and semantic similarity methods to detect ethical risks and hate-speech signals in AI-generated news based on the UNESCO AI Ethics framework.","categories":["NLP","AI Ethics","Machine Learning"],"image":"portfolio-thumbnail-05.png"},"headingText":"Overview","containsRefs":false,"markdown":"\n\n::: {.links}\n üîó[**Source Code (Git)**](https://github.com/annakim9237/ai-generated-news-audit-nlp)\n &nbsp;&nbsp;\n üîó[**Final Report (PDF)**](https://github.com/annakim9237/ai-generated-news-audit-nlp/blob/main/Anna_Kim_CSC_696_Final_Report.pdf)\n  &nbsp;&nbsp;\n üîó[**Final Presentation (PDF)**](https://github.com/annakim9237/ai-generated-news-audit-nlp/blob/main/NLP_project_powerpoint.pdf)\n:::\n\nThe rapid growth of large language models has enabled the generation of highly fluent but potentially misleading synthetic news. As generative AI becomes widely integrated into information ecosystems, the ethical risks embedded in AI-generated narratives‚Äîsuch as subtle fairness violations, toxic framing, biased claims, or manipulative persuasion‚Äîpose significant challenges for public trust, safety, and policy governance. Traditional toxicity classifiers are insufficient for capturing the multi-dimensional ethical harms that can appear in AI-generated content, especially those involving justice, fairness, or structural inequality.\n\nThis project develops a multi-dimensional ethical auditing framework designed specifically for AI-generated fake news. Unlike single-axis toxicity models, this system evaluates each sentence and article across multiple ethical lenses, combining transformer-based classification, semantic similarity mapping, and ethical principle alignment.\n\nBy integrating supervised models with embedding-based reasoning, the pipeline produces:\n\n- multi-label ethical violation detection,\n\n- interpretable UNESCO principle mappings,\n\n- fine-grained justice and toxicity scoring,\n\n- and article-level risk aggregation for policy or moderation workflows.\n\nThe aim is to create a research-grade evaluation pipeline that demonstrates how NLP can operationalize abstract ethical frameworks, enabling more transparent and accountable use of generative AI systems.\n\n---\n\n# System Architecture\n\n## Three-Stage NLP Pipeline\n\n### 1. Justice-Based Ethics Classifier\n- DistilBERT fine-tuned on ETHICS dataset (Hendrycks)\n- Trained on commonsense / justice / combined subsets\n- Detects fairness violations and moral inconsistencies\n- Evaluated distribution shifts across 6 model variants\n\n### 2. Hate Speech & Toxicity Classifier\n- 3-class HateXplain classifier (hate, offensive, normal)\n- Generates probability vectors for each sentence\n- Combined toxicity metric: `p_hate + p_offensive`\n\n### 3. UNESCO Ethics Principle Mapper\n- Sentence-transformer embeddings\n- Cosine similarity mapping to 11 UNESCO AI Ethics principles\n- Produces interpretable ethical-dimension labels\n\n---\n\n# Model Optimization & Ablation\n\n## Techniques\n- Label smoothing (0.1)\n- Dropout (0.2) + attention dropout (0.2)\n- Weight decay\n- Reduced epochs (3 ‚Üí 1)\n- Metrics: F1 score, calibration, confusion matrix\n\n## Model Variants\n- full_v1 / full_v2  \n- commonsense_v1 / commonsense_v2  \n- justice_v1 / justice_v2  \n\n---\n\n# Ethical Risk Categorization\n\n## 4 Final Risk Categories\n- **both** ‚Äî justice + hate/offensive  \n- **justice_only** ‚Äî fairness violations only  \n- **hate_only** ‚Äî toxicity-only  \n- **none** ‚Äî no detected violations  \n\n---\n\n# Article-Level Aggregation\n\n## Metrics\n- Average unethical probability\n- Violation ratios (unethical_ratio, hate_offensive_ratio)\n- UNESCO principle dominance\n- KDE distributions & visualization\n\n---\n\n# Technical Stack\n- PyTorch  \n- HuggingFace Transformers  \n- DistilBERT  \n- Sentence-Transformers  \n- scikit-learn, NumPy, Pandas  \n\n---\n\n# Results\n\n- Multi-axis ethical risk detection system  \n- Differentiates justice-based vs toxicity-based harm  \n- Explainable mapping to UNESCO AI Ethics Principles  \n- Supports systematic content auditing & policy development  \n\nThe final system successfully identifies and differentiates ethical risks across justice-based fairness violations and hate/offensive toxicity. Sentence-level outputs reveal strong model sensitivity to biased or discriminatory framing, especially in cases where the model detects subtle justice-related inequalities without explicit hate speech. The multi-axis structure demonstrates that ethical harm is not monolithic: many sentences that appear non-toxic still fail justice metrics, highlighting the need for multi-dimensional evaluation beyond standard toxicity checks.\n\nThe UNESCO mapping module provides an interpretable layer on top of the classifier outputs, enabling each detected violation to be aligned with principles such as ‚ÄúFairness and Non-Discrimination,‚Äù ‚ÄúHuman Oversight,‚Äù or ‚ÄúDo No Harm.‚Äù This bridging between machine classification and human ethical frameworks makes the system suitable for real-world policy or governance applications.\n\nArticle-level aggregation shows clear thematic clustering:\nsome documents exhibit concentrated justice violations, while others demonstrate heavy toxicity patterns. The combined risk taxonomy effectively distinguishes documents that are both unfair and offensive‚Äîoften the most dangerous form of AI-generated misinformation.\n\nOverall, this system demonstrates that ethical risk in AI-generated fake news is multi-dimensional, hierarchical, and often hidden beneath highly fluent text. The architecture provides a practical foundation for future AI safety tools that must detect manipulation, unfair framing, and harmful narratives embedded within synthetic content. The project highlights both the feasibility and necessity of integrating ethics-aware modeling into modern NLP pipelines.\n\n---\n","srcMarkdownNoYaml":"\n\n::: {.links}\n üîó[**Source Code (Git)**](https://github.com/annakim9237/ai-generated-news-audit-nlp)\n &nbsp;&nbsp;\n üîó[**Final Report (PDF)**](https://github.com/annakim9237/ai-generated-news-audit-nlp/blob/main/Anna_Kim_CSC_696_Final_Report.pdf)\n  &nbsp;&nbsp;\n üîó[**Final Presentation (PDF)**](https://github.com/annakim9237/ai-generated-news-audit-nlp/blob/main/NLP_project_powerpoint.pdf)\n:::\n\n# Overview\nThe rapid growth of large language models has enabled the generation of highly fluent but potentially misleading synthetic news. As generative AI becomes widely integrated into information ecosystems, the ethical risks embedded in AI-generated narratives‚Äîsuch as subtle fairness violations, toxic framing, biased claims, or manipulative persuasion‚Äîpose significant challenges for public trust, safety, and policy governance. Traditional toxicity classifiers are insufficient for capturing the multi-dimensional ethical harms that can appear in AI-generated content, especially those involving justice, fairness, or structural inequality.\n\nThis project develops a multi-dimensional ethical auditing framework designed specifically for AI-generated fake news. Unlike single-axis toxicity models, this system evaluates each sentence and article across multiple ethical lenses, combining transformer-based classification, semantic similarity mapping, and ethical principle alignment.\n\nBy integrating supervised models with embedding-based reasoning, the pipeline produces:\n\n- multi-label ethical violation detection,\n\n- interpretable UNESCO principle mappings,\n\n- fine-grained justice and toxicity scoring,\n\n- and article-level risk aggregation for policy or moderation workflows.\n\nThe aim is to create a research-grade evaluation pipeline that demonstrates how NLP can operationalize abstract ethical frameworks, enabling more transparent and accountable use of generative AI systems.\n\n---\n\n# System Architecture\n\n## Three-Stage NLP Pipeline\n\n### 1. Justice-Based Ethics Classifier\n- DistilBERT fine-tuned on ETHICS dataset (Hendrycks)\n- Trained on commonsense / justice / combined subsets\n- Detects fairness violations and moral inconsistencies\n- Evaluated distribution shifts across 6 model variants\n\n### 2. Hate Speech & Toxicity Classifier\n- 3-class HateXplain classifier (hate, offensive, normal)\n- Generates probability vectors for each sentence\n- Combined toxicity metric: `p_hate + p_offensive`\n\n### 3. UNESCO Ethics Principle Mapper\n- Sentence-transformer embeddings\n- Cosine similarity mapping to 11 UNESCO AI Ethics principles\n- Produces interpretable ethical-dimension labels\n\n---\n\n# Model Optimization & Ablation\n\n## Techniques\n- Label smoothing (0.1)\n- Dropout (0.2) + attention dropout (0.2)\n- Weight decay\n- Reduced epochs (3 ‚Üí 1)\n- Metrics: F1 score, calibration, confusion matrix\n\n## Model Variants\n- full_v1 / full_v2  \n- commonsense_v1 / commonsense_v2  \n- justice_v1 / justice_v2  \n\n---\n\n# Ethical Risk Categorization\n\n## 4 Final Risk Categories\n- **both** ‚Äî justice + hate/offensive  \n- **justice_only** ‚Äî fairness violations only  \n- **hate_only** ‚Äî toxicity-only  \n- **none** ‚Äî no detected violations  \n\n---\n\n# Article-Level Aggregation\n\n## Metrics\n- Average unethical probability\n- Violation ratios (unethical_ratio, hate_offensive_ratio)\n- UNESCO principle dominance\n- KDE distributions & visualization\n\n---\n\n# Technical Stack\n- PyTorch  \n- HuggingFace Transformers  \n- DistilBERT  \n- Sentence-Transformers  \n- scikit-learn, NumPy, Pandas  \n\n---\n\n# Results\n\n- Multi-axis ethical risk detection system  \n- Differentiates justice-based vs toxicity-based harm  \n- Explainable mapping to UNESCO AI Ethics Principles  \n- Supports systematic content auditing & policy development  \n\nThe final system successfully identifies and differentiates ethical risks across justice-based fairness violations and hate/offensive toxicity. Sentence-level outputs reveal strong model sensitivity to biased or discriminatory framing, especially in cases where the model detects subtle justice-related inequalities without explicit hate speech. The multi-axis structure demonstrates that ethical harm is not monolithic: many sentences that appear non-toxic still fail justice metrics, highlighting the need for multi-dimensional evaluation beyond standard toxicity checks.\n\nThe UNESCO mapping module provides an interpretable layer on top of the classifier outputs, enabling each detected violation to be aligned with principles such as ‚ÄúFairness and Non-Discrimination,‚Äù ‚ÄúHuman Oversight,‚Äù or ‚ÄúDo No Harm.‚Äù This bridging between machine classification and human ethical frameworks makes the system suitable for real-world policy or governance applications.\n\nArticle-level aggregation shows clear thematic clustering:\nsome documents exhibit concentrated justice violations, while others demonstrate heavy toxicity patterns. The combined risk taxonomy effectively distinguishes documents that are both unfair and offensive‚Äîoften the most dangerous form of AI-generated misinformation.\n\nOverall, this system demonstrates that ethical risk in AI-generated fake news is multi-dimensional, hierarchical, and often hidden beneath highly fluent text. The architecture provides a practical foundation for future AI safety tools that must detect manipulation, unfair framing, and harmful narratives embedded within synthetic content. The project highlights both the feasibility and necessity of integrating ethics-aware modeling into modern NLP pipelines.\n\n---\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":["cosmo","brand"],"title":"Measuring Ethical Risks in AI-Generated News Using NLP","index":5,"summary":"Built an NLP pipeline using DistilBERT, Transformers, and semantic similarity methods to detect ethical risks and hate-speech signals in AI-generated news based on the UNESCO AI Ethics framework.","categories":["NLP","AI Ethics","Machine Learning"],"image":"portfolio-thumbnail-05.png"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
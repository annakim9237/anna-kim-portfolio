{"title":"Human-Following Robot with Real-Time Face Tracking","markdown":{"yaml":{"title":"Human-Following Robot with Real-Time Face Tracking","subtitle":"Autonomous Person-Following Robot with Face Recognition, Head Pose Estimation & YOLOv8","index":7,"summary":"A real-time identity-aware person-following robot combining OpenCV, head-pose analysis, YOLOv8, and Arduino motor control.","categories":["Computer Vision","Robotics","Python","YOLO","OpenCV"],"image":"./portfolio-thumbnail-07.png"},"headingText":"Overview","containsRefs":false,"markdown":"\n\n::: {.links}\n üîó[**Source Code (Git)**](https://github.com/annakim9237/face-detect-follow-robot/tree/main)\n &nbsp;&nbsp;\n üîó[**Demo Video**](https://drive.google.com/file/d/1utfABIncloF9ycNS5VVR6UQAQ3fVVdho/view?usp=drive_link)\n:::\n\n\nThe Face Detect Follow Robot is an autonomous human-tracking system that recognizes, identifies, and physically follows a specific user in real time. Unlike simple motion-following robots, this project focuses on **identity-aware tracking**, meaning the robot only follows a registered person instead of anyone appearing in front of the camera. This capability is achieved through a hybrid perception system combining classical computer vision, deep-learning detection, head-pose analysis, and embedded robot control.\n\nThe robot begins by registering the target user‚Äôs face using Haar Cascade detection. Thirty aligned face images are collected for identity matching. During operation, the system continuously detects faces, extracts features, compares them with the registered target, and only activates ‚Äúfollow mode‚Äù after multiple consecutive matches ensure stable identification.\n\nTo handle natural human movement‚Äîsuch as turning the head, rotating the face, or temporarily looking away‚Äîthe system integrates **head pose estimation**, adjusting follow direction based on the nose‚Äôs relative position. When the target‚Äôs face becomes occluded or leaves the camera view, the robot switches to **YOLOv8 full-body tracking**, maintaining pursuit until the face reappears. This multimodal pipeline ensures reliable tracking even in difficult visual conditions.\n\nMovement decisions are derived from the target‚Äôs horizontal offset from the frame center and the apparent face size relative to distance. Commands are transmitted to an Arduino-controlled robot via serial communication, enabling smooth forward, backward, and lateral motion.\n\nOverall, this project demonstrates how classical vision, deep-learning detection, and embedded control can be combined to produce a natural, adaptive, human-aware robotic behavior suitable for personal assistance, education, and security applications.\n\n---\n\n<div style=\"text-align: center; margin-top: 20px; margin-bottom: 20px;\">\n  <iframe \n    src=\"https://drive.google.com/file/d/1utfABIncloF9ycNS5VVR6UQAQ3fVVdho/preview\" \n    width=\"720\" \n    height=\"480\" \n    allow=\"autoplay\">\n  </iframe>\n</div>\n\n---\n\n## System Description\n\nThe robot‚Äôs perception pipeline integrates:\n\n- **Face registration** using Haar Cascade detection  \n- **Face alignment** via eye position normalization  \n- **Identity verification** using distance-based comparison  \n- **Head pose estimation** via nose deviation  \n- **YOLOv8 person detection** as fallback when face is not visible  \n- **Serial communication** to coordinate real-time robot motion  \n\nTracking stability is enhanced through:\n\n- Consecutive match filtering  \n- Adaptive thresholds for follow/search states  \n- Dynamic YOLO fallback  \n- Center-tolerance alignment margins  \n\n---\n\n## Technical Architecture\n\n### Computer Vision\n- Haar Cascade face/eye/nose detection  \n- Eye-based rotational alignment  \n- Nose offset for yaw estimation  \n- Distance-based face comparison for ID verification  \n\n### Deep Learning\n- YOLOv8 (Ultralytics) for person detection  \n- Runs every *N* frames for efficiency  \n- Confidence-based switching between modes  \n\n### Robotics Integration\n- Arduino motor controller  \n- Real-time command transmission (PySerial)  \n- Commands: forward, backward, turn left/right, diagonals, stop  \n- Adaptive control based on face size & position  \n\n---\n\n## Applications\n\n- **Personal assistant robots** that follow owners around the home  \n- **Security robots** that identify and track approved individuals  \n- **Educational robotics** for teaching computer vision pipelines  \n- **Interactive robot prototypes** for HRI (Human‚ÄìRobot Interaction) research  \n\n---\n\n## Conclusion\n\nThe Face Detect Follow Robot demonstrates a practical fusion of classical CV algorithms, deep-learning detection models, and embedded robot communication to achieve identity-specific human tracking. Through multimodal perception‚Äîface alignment, head pose estimation, YOLO fallback detection‚Äîthe robot maintains stable pursuit across varied visual conditions and natural human behavior.\n\nThis project highlights the importance of combining **interpretability**, **robustness**, and **real-time performance** in human-centered robotics. While already highly functional, future improvements such as FaceNet embedding-based recognition, ROS navigation, obstacle avoidance, or voice-command integration would further expand its capabilities into a fully autonomous, socially aware robotic system.\n\n---\n","srcMarkdownNoYaml":"\n\n::: {.links}\n üîó[**Source Code (Git)**](https://github.com/annakim9237/face-detect-follow-robot/tree/main)\n &nbsp;&nbsp;\n üîó[**Demo Video**](https://drive.google.com/file/d/1utfABIncloF9ycNS5VVR6UQAQ3fVVdho/view?usp=drive_link)\n:::\n\n## Overview\n\nThe Face Detect Follow Robot is an autonomous human-tracking system that recognizes, identifies, and physically follows a specific user in real time. Unlike simple motion-following robots, this project focuses on **identity-aware tracking**, meaning the robot only follows a registered person instead of anyone appearing in front of the camera. This capability is achieved through a hybrid perception system combining classical computer vision, deep-learning detection, head-pose analysis, and embedded robot control.\n\nThe robot begins by registering the target user‚Äôs face using Haar Cascade detection. Thirty aligned face images are collected for identity matching. During operation, the system continuously detects faces, extracts features, compares them with the registered target, and only activates ‚Äúfollow mode‚Äù after multiple consecutive matches ensure stable identification.\n\nTo handle natural human movement‚Äîsuch as turning the head, rotating the face, or temporarily looking away‚Äîthe system integrates **head pose estimation**, adjusting follow direction based on the nose‚Äôs relative position. When the target‚Äôs face becomes occluded or leaves the camera view, the robot switches to **YOLOv8 full-body tracking**, maintaining pursuit until the face reappears. This multimodal pipeline ensures reliable tracking even in difficult visual conditions.\n\nMovement decisions are derived from the target‚Äôs horizontal offset from the frame center and the apparent face size relative to distance. Commands are transmitted to an Arduino-controlled robot via serial communication, enabling smooth forward, backward, and lateral motion.\n\nOverall, this project demonstrates how classical vision, deep-learning detection, and embedded control can be combined to produce a natural, adaptive, human-aware robotic behavior suitable for personal assistance, education, and security applications.\n\n---\n\n<div style=\"text-align: center; margin-top: 20px; margin-bottom: 20px;\">\n  <iframe \n    src=\"https://drive.google.com/file/d/1utfABIncloF9ycNS5VVR6UQAQ3fVVdho/preview\" \n    width=\"720\" \n    height=\"480\" \n    allow=\"autoplay\">\n  </iframe>\n</div>\n\n---\n\n## System Description\n\nThe robot‚Äôs perception pipeline integrates:\n\n- **Face registration** using Haar Cascade detection  \n- **Face alignment** via eye position normalization  \n- **Identity verification** using distance-based comparison  \n- **Head pose estimation** via nose deviation  \n- **YOLOv8 person detection** as fallback when face is not visible  \n- **Serial communication** to coordinate real-time robot motion  \n\nTracking stability is enhanced through:\n\n- Consecutive match filtering  \n- Adaptive thresholds for follow/search states  \n- Dynamic YOLO fallback  \n- Center-tolerance alignment margins  \n\n---\n\n## Technical Architecture\n\n### Computer Vision\n- Haar Cascade face/eye/nose detection  \n- Eye-based rotational alignment  \n- Nose offset for yaw estimation  \n- Distance-based face comparison for ID verification  \n\n### Deep Learning\n- YOLOv8 (Ultralytics) for person detection  \n- Runs every *N* frames for efficiency  \n- Confidence-based switching between modes  \n\n### Robotics Integration\n- Arduino motor controller  \n- Real-time command transmission (PySerial)  \n- Commands: forward, backward, turn left/right, diagonals, stop  \n- Adaptive control based on face size & position  \n\n---\n\n## Applications\n\n- **Personal assistant robots** that follow owners around the home  \n- **Security robots** that identify and track approved individuals  \n- **Educational robotics** for teaching computer vision pipelines  \n- **Interactive robot prototypes** for HRI (Human‚ÄìRobot Interaction) research  \n\n---\n\n## Conclusion\n\nThe Face Detect Follow Robot demonstrates a practical fusion of classical CV algorithms, deep-learning detection models, and embedded robot communication to achieve identity-specific human tracking. Through multimodal perception‚Äîface alignment, head pose estimation, YOLO fallback detection‚Äîthe robot maintains stable pursuit across varied visual conditions and natural human behavior.\n\nThis project highlights the importance of combining **interpretability**, **robustness**, and **real-time performance** in human-centered robotics. While already highly functional, future improvements such as FaceNet embedding-based recognition, ROS navigation, obstacle avoidance, or voice-command integration would further expand its capabilities into a fully autonomous, socially aware robotic system.\n\n---\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":["cosmo","brand"],"title":"Human-Following Robot with Real-Time Face Tracking","subtitle":"Autonomous Person-Following Robot with Face Recognition, Head Pose Estimation & YOLOv8","index":7,"summary":"A real-time identity-aware person-following robot combining OpenCV, head-pose analysis, YOLOv8, and Arduino motor control.","categories":["Computer Vision","Robotics","Python","YOLO","OpenCV"],"image":"./portfolio-thumbnail-07.png"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}